# ML models
if ( !require("mgcv") ) { install.packages("mgcv"); library("mgcv") } # to run generalized additive model (GAM) algorithm
if ( !require("gam") ) { install.packages("gam"); library("gam") } # to run generalized additive model (GAM) algorithm
## if ( !require("fastAdaboost") ) { install.packages("fastAdaboost"); library("fastAdaboost") } # to run adaboost ml algorithm
if ( !require("kernlab") ) { install.packages("kernlab"); library("kernlab") } # to run support vector machine (svm) algorithm
## if ( !require("earth") ) { install.packages("earth"); library("earth") } # to run MARS ml algorithm
if ( !require("randomForest") ) { install.packages("randomForest"); library("randomForest") } # to run random forest (RF)
## if ( !require("RRF") ) { install.packages("RRF"); library("RRF") } # to run RF and additional features
if(!server){ # packages having problems on the server
if( !require("xgboost") ) { install.packages("xgboost"); library("xgboost") } # to run Boosted Classification Trees
if( !require("ada") ) { install.packages("ada"); library("ada") } # to run Boosted Classification Trees
}
# have to be loaded at the end to not cache function 'train'
if ( !require("caret") ) { install.packages("caret"); library("caret") } # comprehensive framework to build machine learning models
## Load functions ####
source("ml_model_functions.r")
source("stat_model_functions.r")
source("plot_functions.r")
source("utilities.r")
if(run.ann){ source("ann_model_functions.r")}
## Load data ####
# Define directory and files
dir.env.data      <- "../../Data/Processed data/Environmental data/"
dir.inv.data      <- "../../Data/Processed data/Invertebrate data/"
dir.orig.data     <- "../../Data/Original data/"
dir.workspace     <- "../Intermediate results/"
dir.plots.output  <- "../Plots/Models analysis plots/"
dir.expl.plots.output <- "../Plots/Explorative plots/"
dir.models.output <- "../Intermediate results/Trained models/"
file.env.data     <- "environmental_data_2020-06-25.dat"
# file.env.data.lme <- "environmental_data_lme_2020-06-25.dat"
# file.inv.data     <- "occ_data_2020-06-25.dat"
file.inv.data     <- "occ_data_IBCH_2020-06-25.dat"
file.ibch         <- "ListTaxaIBCH2019.csv"
# file.prev         <- "prevalence_2020-06-25.dat"
file.prev         <- "prevalence_IBCH_2020-06-25.dat"
file.stations     <- "temperature_stations.dat"
file.stations.used     <- "temperature_dataset_11_07.rds"
# Load datasets
data.env          <- read.delim(paste0(dir.env.data, file.prefix, file.env.data),header=T,sep="\t", stringsAsFactors=T)
data.inv          <- read.delim(paste0(dir.inv.data, file.prefix, file.inv.data),header=T,sep="\t", stringsAsFactors=F)
taxa.ibch         <- read.csv(paste0(dir.inv.data, file.ibch), header = T, sep = ";", stringsAsFactors = F)
prev.inv          <- read.delim(paste0(dir.inv.data, file.prefix, file.prev),header=T,sep="\t", stringsAsFactors=F)
# Prepare inputs for geographic plots
inputs <- map.inputs(dir.env.data = dir.env.data, data.env = data.env)
?groupKFold
q
# to display chunk code along with its results
knitr::opts_chunk$set(echo = TRUE)
# load required packages:
# to conduct the exercises:
if ( !require("ecosim") ) {install.packages("ecosim"); library("ecosim") }
if ( !require("stoichcalc") ) {install.packages("stoichcalc"); library("stoichcalc") }
if ( !require("deSolve") ) {install.packages("deSolve"); library("deSolve") }
# to work with the R Markdown format:
if ( !require("markdown") ) {install.packages("markdown"); library("markdown") }
# Model with constant driving forces
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# definition of model parameters:
param    <- list(k.gro.ALG   = 0.5,      # 1/d
k.death.ALG = 0.1,      # 1/d
K.HPO4      = 0.002,    # gP/m3
alpha.P.ALG = 0.003,    # gP/gDM
A           = 5e+006,   # m2
h.epi       = 5,        # m
Q.in        = 5,        # m3/s
C.HPO4.in   = 0.04,     # gP/m3
C.HPO4.ini  = 0.004,    # gP/m3
C.ALG.ini   = 0.1)      # gDM/m3
# definition of right-hand side of differential equations (11.8, 11.9):
rhs <- function(t,y,par)
{
# equation (11.8):
dC.HPO4_dt <-   par$Q.in*86400/(par$h.epi*par$A) * (par$C.HPO4.in - y["C.HPO4"]) -
par$alpha.P.ALG * par$k.gro.ALG * y["C.HPO4"] /
(par$K.HPO4 + y["C.HPO4"]) * y["C.ALG"]
# equation (11.9): TO BE COMPLETED
dC.ALG_dt  <-  - par$Q.in*86400/(par$h.epi*...) * y["C.ALG"] +
par$k.gro.ALG * y["C.HPO4"] / (par$K.HPO4 + y["C.HPO4"]) * ... -
... * y["C.ALG"]
return(list(c(dC.HPO4_dt,dC.ALG_dt)))
}
# to display chunk code along with its results
knitr::opts_chunk$set(echo = TRUE)
# definition of right-hand side of differential equations (11.8, 11.9):
rhs <- function(t,y,par)
{
# equation (11.8):
dC.HPO4_dt <-   par$Q.in*86400/(par$h.epi*par$A) * (par$C.HPO4.in - y["C.HPO4"]) -
par$alpha.P.ALG * par$k.gro.ALG * y["C.HPO4"] /
(par$K.HPO4 + y["C.HPO4"]) * y["C.ALG"]
# equation (11.9): TO BE COMPLETED
dC.ALG_dt  <-   - par$Q.in*86400/(par$h.epi*par$A) * y["C.ALG"] +
par$k.gro.ALG * y["C.HPO4"] / (par$K.HPO4 + y["C.HPO4"]) * y["C.ALG"] -
par$k.death.ALG * y["C.ALG"]
return(list(c(dC.HPO4_dt,dC.ALG_dt)))
}
# read help file of the solver to understand the arguments needed:
?ode
# solve differential equations:
res <- ode(y=c(C.HPO4=param$C.HPO4.ini,C.ALG=param$C.ALG.ini),
times=seq(0,365,by=1),func=rhs,par=param)
# plot results:
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
# plot results to a file:
file.name <- "exercise_1_results_a1_deSolve.pdf"
# open a pdf file to store the plots
pdf(file.name, paper = 'special', width = 10, height = 5, onefile = TRUE)
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
dev.off() # close the pdf file
# plot results:
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
# plot results to a file:
file.name <- "exercise_1_results_a1_deSolve.pdf"
# open a pdf file to store the plots
pdf(file.name, paper = 'special', width = 10, height = 5, onefile = TRUE)
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
dev.off() # close the pdf file
# plot results:
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
# plot results to a file:
file.name <- "exercise_1_results_a1_deSolve.pdf"
# open a pdf file to store the plots
pdf(file.name, paper = 'special', width = 10, height = 5, onefile = TRUE)
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
dev.off() # close the pdf file
# plot results:
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
# plot results to a file:
file.name <- "exercise_1_results_a1_deSolve.pdf"
# open a pdf file to store the plots
pdf(file.name, paper = 'special', width = 10, height = 5, onefile = TRUE)
par(mfrow=c(1,2))
plot(res[,"time"],res[,"C.HPO4"],type="l",xlab="t",ylab="C.HPO4",main="C.HPO4")
plot(res[,"time"],res[,"C.ALG"] ,type="l",xlab="t",ylab="C.ALG" ,main="C.ALG")
dev.off() # close the pdf file
5e+006
5e+006*5
5e+006*5*param$C.HPO4.ini
?MCMC
??MCMC
# to display chunk code along with its results
knitr::opts_chunk$set(echo = TRUE)
# load required packages:
if ( !require("ecosim") ) {install.packages("ecosim"); library("ecosim") }
if ( !require("adaptMCMC") ) {install.packages("adaptMCMC"); library("adaptMCMC") }
# Model with constant driving forces
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# definition of model parameters:
param    <- list(k.gro.ALG   = 0.5,      # 1/d
k.death.ALG = 0.1,      # 1/d
K.HPO4      = 0.002,    # gP/m3
alpha.P.ALG = 0.003,    # gP/gDM
A           = 5e+006,   # m2
h.epi       = 5,        # m
Q.in        = 5,        # m3/s
C.HPO4.in   = 0.04,     # gP/m3
C.HPO4.ini  = 0.004,    # gP/m3
C.ALG.ini   = 0.1)      # gDM/m3
# definition of transformation processes
# growth of algae:
gro.ALG   <- new(Class  = "process",
name   = "Growth of algae",
rate   = expression(k.gro.ALG
*C.HPO4/(K.HPO4+C.HPO4)
*C.ALG),
stoich = list(C.ALG  = expression(1),              # gDM/gDM
C.HPO4 = expression(-alpha.P.ALG)))  # gP/gDM
# death of algae:
death.ALG <- new(Class = "process",
name   = "Death of algae",
rate   = expression(k.death.ALG*C.ALG),
stoich = list(C.ALG  = expression(-1)))            # gDM/gDM
# definition of reactor to describe the epilimnion of the lake:
epilimnion <-
new(Class            = "reactor",
name             = "Epilimnion",
volume.ini       = expression(A*h.epi),
conc.pervol.ini  = list(C.HPO4 = expression(C.HPO4.ini),     # gP/m3
C.ALG  = expression(C.ALG.ini)),     # gDM/m3
inflow           = expression(Q.in*86400),                   # m3/d
inflow.conc      = list(C.HPO4 = expression(C.HPO4.in),
C.ALG  = 0),
outflow          = expression(Q.in*86400),
processes        = list(gro.ALG,death.ALG))
# definition of the system consisting of a single reactor:
system <- new(Class    = "system",
name     = "Lake",
reactors = list(epilimnion),
param    = param,
t.out    = seq(0,2*365,by=1))
# perform simulation:
res.11.1 <- calcres(system)
# plot results whit default options:
plotres(res.11.1, colnames=list("C.HPO4","C.ALG"))
# Task 1:    #############################################################################################
# Simulation of the simple lake phytoplankton model with consideration of parameter uncertainty
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# set a number of samples we want to draw to forward simulate the uncertainty of the parameters in the model
N <- 20  # probably not enough!
# initialize a list for the result of the model for each sample forward simulation
res.parunc  <- list()
for ( i in 1:N ){
# draw parameters from the log-normal distributions
param.unc <- param
param.unc[["k.gro.ALG"]] <- randnorm(mean = param[["k.gro.ALG"]],
sd = 0.05, log = TRUE)
param.unc[["k.death.ALG"]] <- randnorm(mean = param[["k.death.ALG"]],
sd = 0.05, log = TRUE)
param.unc[["K.HPO4"]] <- randnorm(mean = param[["k.death.ALG"]],
sd = 0.05, log = TRUE)
# simulate and save model results with new parameter values
system@param <- param.unc
res.parunc[[i]] <- calcres(system)
}
# plot results of all simulations with parameters uncertainty together
plotres(res      = res.parunc,
colnames = list("C.HPO4","C.ALG"))
# plot results of all simulations with parameters uncertainty together
plotres(res      = res.parunc,
colnames = list("C.HPO4","C.ALG"))
View(res.parunc)
randnorm(mean = param[["k.gro.ALG"]],
sd = 0.05, log = TRUE)
test <- res.parunc[[1]]
View(test)
1:N
View(test)
# Compute the mean and sd of the outputs at t = 365
res.HPO4.365 <- c()
res.ALG.365 <- c()
View(test)
for(i in 1:N){
res.HPO4.365[i] <- res.parunc[[i]][365,"C.HPO4"]
res.ALG.365[i] <- res.parunc[[i]][365,"C.ALG"]
}
# Compute the mean and sd of the outputs at t = 365
mean(res.HPO4.365)
sd(res.HPO4.365)
mean(res.ALG.365)
mean(res.ALG.365)
sd(res.ALG.365)
# Make a histogram of the model outputs at t = 365
hist(res.HPO4.365)
hist(res.ALG.365)
hist(res.ALG.365, main = "Histogram of C.ALG at t = 365")
# Make a histogram of the model outputs at t = 365
hist(res.HPO4.365, main = "Histogram of C.HPO4 at t = 365", xlab = "C.HPO4")
hist(res.ALG.365, main = "Histogram of C.ALG at t = 365", xlab = "C.ALG")
# load data
file.name <- "Observations_Concentration_HPO4_ALG.csv"
observations <- read.csv(file.name, header = T, sep = ",", stringsAsFactors = F)
View(observations)
str(observations)
randnorm(mean = param[["k.gro.ALG"]],
sd = 0.05, log = TRUE)
param[["k.gro.ALG"]]
View(res.parunc)
# plot results of all simulations with parameters uncertainty together
plotres(res      = res.parunc,
colnames = list("C.HPO4","C.ALG"))
test <- res.parunc[[1]]
View(test)
View(res.parunc)
i <- 1
res.parunc[[i]]
# First extract the results at t = 365
res.HPO4.365 <- c()
res.ALG.365 <- c()
for(i in 1:N){
# i <- 1
res.HPO4.365[i] <- res.parunc[[i]][365,"C.HPO4"]
res.ALG.365[i] <- res.parunc[[i]][365,"C.ALG"]
}
res.HPO4.365
# Compute the mean and sd of the outputs at t = 365
mean(res.HPO4.365)
sd(res.HPO4.365)
# Make a histogram of the model outputs at t = 365
hist(res.HPO4.365, main = "Histogram of C.HPO4 at t = 365", xlab = "C.HPO4")
# set a number of samples we want to draw to forward simulate the uncertainty of the parameters in the model
N <- 50  # probably not enough!
# initialize a list for the result of the model for each sample forward simulation
res.parunc  <- list()
for ( i in 1:N ){
# draw parameters from the log-normal distributions
param.unc <- param
param.unc[["k.gro.ALG"]] <- randnorm(mean = param[["k.gro.ALG"]],
sd = 0.05, log = TRUE)
param.unc[["k.death.ALG"]] <- randnorm(mean = param[["k.death.ALG"]],
sd = 0.05, log = TRUE)
param.unc[["K.HPO4"]] <- randnorm(mean = param[["k.death.ALG"]],
sd = 0.05, log = TRUE)
# simulate and save model results with new parameter values
system@param <- param.unc
res.parunc[[i]] <- calcres(system)
}
for ( i in 1:N ){
# draw parameters from the log-normal distributions
param.unc <- param
param.unc[["k.gro.ALG"]] <- randnorm(mean = param[["k.gro.ALG"]],
sd = 0.05, log = TRUE)
param.unc[["k.death.ALG"]] <- randnorm(mean = param[["k.death.ALG"]],
sd = 0.05, log = TRUE)
param.unc[["K.HPO4"]] <- randnorm(mean = param[["k.death.ALG"]],
sd = 0.05, log = TRUE)
# simulate and save model results with new parameter values
system@param <- param.unc
res.parunc[[i]] <- calcres(system)
}
# plot results of all simulations with parameters uncertainty together
plotres(res      = res.parunc,
colnames = list("C.HPO4","C.ALG"))
test <- res.parunc[[1]]
# First extract the results at t = 365
res.HPO4.365 <- c()
res.ALG.365 <- c()
for(i in 1:N){
# i <- 1
res.HPO4.365[i] <- res.parunc[[i]][365,"C.HPO4"]
res.ALG.365[i] <- res.parunc[[i]][365,"C.ALG"]
}
# Compute the mean and sd of the outputs at t = 365
mean(res.HPO4.365)
sd(res.HPO4.365)
mean(res.ALG.365)
sd(res.ALG.365)
# Make a histogram of the model outputs at t = 365
hist(res.HPO4.365, main = "Histogram of C.HPO4 at t = 365", xlab = "C.HPO4")
hist(res.ALG.365, main = "Histogram of C.ALG at t = 365", xlab = "C.ALG")
# Make a histogram of the model outputs at t = 365
hist(res.HPO4.365, main = "Histogram of C.HPO4 at t = 365", xlab = "C.HPO4")
observations <- read.csv(file.name, header = T,
sep = ",", stringsAsFactors = )
View(observations)
# Formulation of a likelihood function for the lake plankton model
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
loglikeli <- function(par, system, obs, verbose=FALSE){
# negative parameter values lead to a likelihood of zero or a log likelihood of
# minus infinity:
if ( any(par<=0) ) return(-Inf)
# set the parameters equal to the current values given as the first function argument
# (keep the other parameters):
system@param[names(par)] <- par
# set the start time to zero and the other output times to those with observations:
system@t.out <- c(0,as.numeric(rownames(obs)))
# calculate the deterministic results of our model:
res <- calcres(system)
# calculate the log likelihood using independent, normal distributions
# with extracting the standard deviations from the parameter vector
ll <-
sum(c(dnorm(x=obs[,"C.HPO4"], mean=res[-1,"C.HPO4"], sd=par["sd.obs.HPO4"], log=TRUE),
dnorm(x=obs[,"C.ALG"] , mean=res[-1,"C.ALG"] , sd=par["sd.obs.ALG"], log=TRUE)))
# print parameters and likelihood if the verbose mode was selected:
if ( verbose ) { print(par); cat("loglikeli =",ll,"\n") }
# return the log likelihood value
return(ll)
}
par.ini <- c(k.gro.ALG = 0.5, k.death.ALG = 0.1, K.HPO4 = 0.002,
sd.obs.HPO4 = 0.004, sd.obs.ALG = 0.02)
loglikeli(par=par.ini, system=system, obs=observations, verbose=TRUE)
# Task 3:    #############################################################################################
# Maximum likelihood parameter estimation
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
res.optim <- optim(par = par.ini, fn = loglikeli,
method = "BFGS", control = list(fnscale=-1, maxit=300),
system = system, obs = observations, verbose = FALSE)
# load data
file.name <- "Observations_Concentration_HPO4_ALG.csv"
observations <- read.csv(file.name, # header = T,
sep = ",", stringsAsFactors = )
rownames(observations) <- observations$Day # replace row names of the dataframe to match the structure needed for plotres
# plot data
plotres(res      = res.11.1,
colnames = "C.HPO4")
points(observations$Day, observations$C.HPO4, col=2)
plotres(res      = res.11.1,
colnames = "C.ALG")
points(observations$Day, observations$C.ALG, col=2)
# Formulation of a likelihood function for the lake plankton model
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
loglikeli <- function(par, system, obs, verbose=FALSE){
# negative parameter values lead to a likelihood of zero or a log likelihood of
# minus infinity:
if ( any(par<=0) ) return(-Inf)
# set the parameters equal to the current values given as the first function argument
# (keep the other parameters):
system@param[names(par)] <- par
# set the start time to zero and the other output times to those with observations:
system@t.out <- c(0,as.numeric(rownames(obs)))
# calculate the deterministic results of our model:
res <- calcres(system)
# calculate the log likelihood using independent, normal distributions
# with extracting the standard deviations from the parameter vector
ll <-
sum(c(dnorm(x=obs[,"C.HPO4"], mean=res[-1,"C.HPO4"], sd=par["sd.obs.HPO4"], log=TRUE),
dnorm(x=obs[,"C.ALG"] , mean=res[-1,"C.ALG"] , sd=par["sd.obs.ALG"], log=TRUE)))
# print parameters and likelihood if the verbose mode was selected:
if ( verbose ) { print(par); cat("loglikeli =",ll,"\n") }
# return the log likelihood value
return(ll)
}
par.ini <- c(k.gro.ALG = 0.5, k.death.ALG = 0.1, K.HPO4 = 0.002,
sd.obs.HPO4 = 0.004, sd.obs.ALG = 0.02)
loglikeli(par=par.ini, system=system, obs=observations, verbose=TRUE)
# Task 3:    #############################################################################################
# Maximum likelihood parameter estimation
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
res.optim <- optim(par = par.ini, fn = loglikeli,
method = "BFGS", control = list(fnscale=-1, maxit=300),
system = system, obs = observations, verbose = FALSE)
print("maximum likelihood solution:")
print(round(res.optim$par, 4))
print("original parameter values:")
print(par.ini)
# Task 4:    #############################################################################################
# Bayesian parameter estimation
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
logprior <- function(par){
# the prior density is zero for negative values, its log is minus infinity:
if ( any(par<0) ) return(-Inf)
# define priors for all parameters that we want to infer
sum(dnorm(x=par["k.gro.ALG"], mean=0.5,sd=0.1,log=TRUE),
dnorm(x=par["k.death.ALG"], mean=0.1,sd=0.01,log=TRUE),
dnorm(x=par["K.HPO4"], mean=0.002,sd=0.0002,log=TRUE),
dnorm(x=par["sd.obs.HPO4"], mean=0.04,sd=0.004,log=TRUE),
dnorm(x=par["sd.obs.ALG"], mean=0.02,sd=0.002,log=TRUE)
)
}
# logposterior = logprior + loglikelihood
logposterior = function(par, obs, system, verbose=FALSE){
lp = logprior(par)
if(is.infinite(lp)){
return(-Inf) # then sampler will go away from these param values (instead of crashing)
} else {
lp = lp + loglikeli(par, system=system, obs=obs, verbose=verbose)
return(lp)
}
}
par.ini <- c(k.gro.ALG = 0.5, k.death.ALG = 0.1, K.HPO4 = 0.002,
sd.obs.HPO4 = 0.004, sd.obs.ALG = 0.02)
# test postior function
logprior(par = par.ini)
logposterior(par = par.ini, system=system, obs=observations)
# run sampler
sampsize <- 1000
res.mcmc <- MCMC(logposterior, n=sampsize,
system=system, obs=observations,
init = par.ini,  acc.rate = 0.234,
scale = 0.00001*par.ini)
res.mcmc$acceptance.rate
par(mfrow=c(2, 3))
for(i in 1:ncol(res.mcmc$samples)) {
plot(res.mcmc$samples[,i], type="l", main=colnames(res.mcmc$samples)[i], xlab="iteration"[i])
}
setwd("C:/Users/cholleem/Documents/ArtificialData_Streambugs_SDMs/B1_scripts_sdms")
###############################################################################|
#                                                                              #
#  --- Study of the influence of noise on overfitting of machine learning ---  #
#            --- and statistical species distribution models. ---              #
#                                                                              #
#                          --- February 2023 ---                               #
#                                                                              #
#                   --- Emma Chollet, Gaspard FragniÃ¨re ---                    #
#                --- Andreas Scheidegger and Nele Schuwirth ---                #
#                                                                              #
#                      --- emma.chollet@eawag.ch ---                           #
#                                                                              #
###############################################################################|
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PRELIMINARIES ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Sys.setenv(LANG="EN")
set.seed(13)   # Always set seed to a lucky number
getwd()        # Show working directory. It needs to be the location of 'main.r'
rm(list=ls())  # Free work space
graphics.off() # Clean graphics display
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Libraries ####
if (!require("dplyr")){install.packages("dplyr"); library("dplyr")}                    # to sort, join, merge data
if (!require("tidyr") ) { install.packages("tidyr"); library("tidyr") }               # to sort, join, merge data
if (!require("ggplot2")){install.packages("ggplot2"); library("ggplot2")}                    # to sort, join, merge data
if (!require("readr")){install.packages("readr"); library("readr")}
if (!require("jsonlite")){install.packages("jsonlite"); library("jsonlite")}
if (!require("pROC")){install.packages("pROC"); library("pROC")}  # to compute AUC
if (!require("sf") ) { install.packages("sf"); library("sf") }                        # to read layers for plotting maps
if (!require("ggpattern")){install.packages("ggpattern"); library("ggpattern")}        # to add patterns in boxplots
# specific for Neural Network
if (!require("reticulate")){install.packages("reticulate"); library("reticulate")}
# install_miniconda()              # run this the very first time reticulate is installed
# install.packages("tensorflow")
library("tensorflow")
# virtualenv_install("C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow", "tensorflow==2.16")
# install_tensorflow(envname = "C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow")
install_tensorflow()             # run this line only when opening new R session
# install.packages("keras")
library("keras")
install_keras()                  # run this line only when opening new R session
