# replace 0/1 by absent/present
for (taxon in taxa.colnames) {
data.input[which(data.input[,taxon] == 0),taxon] <- "Absent"
data.input[which(data.input[,taxon] == 1),taxon] <- "Present"
data.input[,taxon] = as.factor(data.input[,taxon])
}
# make long dataframe for analysis
long.df.input.taxa <- gather(data.input, key = Taxon, value = Observation, all_of(taxa.colnames))
long.df.input.taxa$Taxon <- gsub("Occurrence.", "", long.df.input.taxa$Taxon)
long.df.input.env.taxa <- gather(long.df.input.taxa, key = Factor, value = Value, all_of(env.factor))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Prevalence analysis ####
# plot bars pres/abs/NA
plot.data <- long.df.input.taxa
plot.data$Observation <- factor(plot.data$Observation, levels=c("Present", NA, "Absent"))
temp.taxa <- unique(plot.data$Taxon)
temp.catch <- unique(plot.data$Watershed)
# analyze distribution of prevalence and NAs across catchments
p <- ggplot(plot.data, aes(x = Taxon, fill = Observation))
p <- p + geom_bar(stat="count", position='stack')
p <- p + facet_wrap(~ Watershed, scales = "free_y")
p <- p + scale_fill_manual(values=vect.col.pres.abs)
p <- p + theme_bw()
p <- p + scale_x_discrete(limits = temp.taxa)
p <- p + theme(axis.text.x = element_text(angle=90))
p
# analyze distribution of prevalence and NAs across taxa
q <- ggplot(plot.data, aes(x = Watershed, fill = Observation))
q <- q + geom_bar(stat = "count", position = "stack")
q <- q + facet_wrap(~ Taxon, scales = "free_y")
q <- q + scale_fill_manual(values=vect.col.pres.abs)
q <- q + theme_bw()
q <- q + scale_x_discrete(limits = temp.catch)
q <- q + theme(axis.text.x = element_text(angle=90))
q
# print plots in pdf
file.name <- "GeomBar_PresAbs_TaxaCatch"
print.pdf.plots(list.plots = list(q,p), width = 12, height = 9, dir.output = dir.experiment, info.file.name = "", file.name = file.name,
png = F)
dev.off()
###############################################################################|
#                                                                              #
#  --- Study of the influence of noise on overfitting of machine learning ---  #
#            --- and statistical species distribution models. ---              #
#                                                                              #
#                          --- February 2023 ---                               #
#                                                                              #
#                   --- Emma Chollet, Gaspard FragniÃ¨re ---                    #
#                --- Andreas Scheidegger and Nele Schuwirth ---                #
#                                                                              #
#                      --- emma.chollet@eawag.ch ---                           #
#                                  l                                           #
###############################################################################|
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PRELIMINARIES ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Sys.setenv(LANG="EN")
set.seed(13)   # Always set seed to a lucky number
getwd()        # Show working directory. It needs to be the location of 'main.r'
rm(list=ls())  # Free work space
graphics.off() # Clean graphics display
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Libraries ####
if (!require("dplyr")){install.packages("dplyr"); library("dplyr")}                    # to sort, join, merge data
if ( !require("tidyr") ) { install.packages("tidyr"); library("tidyr") }               # to sort, join, merge data
if (!require("readr")){install.packages("readr"); library("readr")}
if (!require("jsonlite")){install.packages("jsonlite"); library("jsonlite")}
if (!require("pROC")){install.packages("pROC"); library("pROC")}  # to compute AUC
if ( !require("sf") ) { install.packages("sf"); library("sf") }                        # to read layers for plotting maps
# specific for Neural Network
if (!require("reticulate")){install.packages("reticulate"); library("reticulate")}
#install_miniconda()              # run this the very first time reticulate is installed
#install.packages("tensorflow")
library("tensorflow")
#install_tensorflow()             # run this line only when opening new R session
#install.packages("keras")
library("keras")
#install_keras()                  # run this line only when opening new R session
#use_condaenv()
# reticulate::install_python(version = '<version>')
# caret has to be loaded at the end to not cache function 'train'
if (!require("caret")){install.packages("caret"); library("caret")}
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Directories, files and functions ####
# define directories
dir.input.data          <- "../A3_outputs_streambugs/"
dir.output              <- "../B2_outputs_sdms/"
dir.utilities           <- "../00_utilities/"
# dir.create(dir.output)
# define files
name.streambugs.run     <- "10Catch_3081Sites_Prev0.15_10Yea_3651Steps"
file.input.data         <- paste0(name.streambugs.run, "_WideData_ResultsThreshPresAbs.csv")
file.prev.taxa          <- paste0(name.streambugs.run, "_PrevalenceTaxonomy_ThreshPresAbs.csv")
file.selected.taxa      <- "selected_taxa_analysis.csv"
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Load data and functions ####
# read data
data.env.taxa          <- read.csv(paste0(dir.input.data, file.input.data), header = T, sep = ";", stringsAsFactors = F)
data.prev.taxa      <- read.csv(paste0(dir.input.data, file.prev.taxa), header = T, sep = ";", stringsAsFactors = F)
data.selected.taxa  <- read.csv(paste0(dir.utilities, file.selected.taxa), header = T, sep = ";", stringsAsFactors = F)
# load functions
source(paste0(dir.utilities, "utilities_global.r"))
source("performances_assessment.r")
source("training_pipeline.r")
source("ml_models.r")
source("plotting.r")
# prepare inputs for plotting results on swiss maps
map.inputs      <- map.inputs(directory = paste0(dir.utilities,"swiss.map.gdb"))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Inputs, training options and noise scenarios ####
# environmental factors used for streambugs
vect.dir.env.fact <- c("Temperature"      = "tempmaxC",
"Flow velocity"    = "currentms",
"Toxic units"      = "orgmicropollTU",
"Saproby"          = "saprowqclass")
vect.indir.env.fact <- c("Shade"          = "shade",
"Litter input"   = "Lit_Inp" ,
"Phosphorus"     = "C_P",
"Nitrate"        = "C_N")
vect.info <- colnames(data.env.taxa)[1:5]
print(vect.info)
# environmental factors selected for sdms
env.factor <- vect.dir.env.fact
env.factor.full <- env.factor
no.env.fact <- length(env.factor)
# taxa list full
cind.taxa <- which(grepl("Occurrence.", colnames(data.env.taxa)))
vect.taxa.full <- colnames(data.env.taxa)[cind.taxa]
names(vect.taxa.full) <- gsub("Occurrence.", "", vect.taxa.full)
no.taxa.full <- length(vect.taxa.full)
print(vect.taxa.full)
# taxa list selected for analysis and above prevalence threshold
prev.threshold <- 0.1
temp.occ.taxa <- data.prev.taxa[which(data.prev.taxa$Prevalence > prev.threshold), "Occurrence.taxa"]
selected.taxa.analysis <- data.selected.taxa$Occurrence.taxa
taxa.colnames <- selected.taxa.analysis[which(selected.taxa.analysis %in% temp.occ.taxa)]
names(taxa.colnames) <- gsub("Occurrence.", "", taxa.colnames)
no.taxa <- length(taxa.colnames)
print(taxa.colnames)
# select colors for present/absent plots
vect.col.pres.abs <- c( "Present" = "#00BFC4", "Absent" = "#F8766D")
scales::show_col(vect.col.pres.abs)
# sdms training options
number.split            <- 3
split.criterion         <- "ReachID"
number.sample           <- 1000
sdm.models              <- list("GLM" = "glm",
"GAM" = "gamLoess",
"RF"  = "rf")#,
# "ann")
no.models <- length(sdm.models)
models <- append(list("null"), sdm.models)
# noise scenarios
list.noise <- list(
# "noise.disp"     = list("type"       = "dispersal",
#                        "target"     = NULL,
#                        "amount"     = NULL,
#                        "parameters" = NULL) # ,
# "noise.temp"     = list("type"       = "gaussian",
#                         "target"     = "temperature",
#                         "amount"     = 5,
#                         "parameters" = list("min"=0, "max"=35)),
#
# "noise.gamm"     = list("type"       = "missdetection",
#                         "target"     = "Occurrence.Gammaridae",
#                         "amount"     = 0.1,
#                         "parameters" = NULL),
#
# "noise.add.fact" = list("type"       = "add_factor",
#                         "target"     = "random1", # new factor name
#                         "amount"     = NULL,
#                         "parameters" = NULL),
#
# "noise.rem.fact" = list("type"       = "remove_factor",
#                         "target"     = "temperature",
#                         "amount"     = NULL,
#                         "parameters" = NULL),
)
# define experiment name
experiment.name <- paste0(
number.sample, "Sites_",
no.taxa, "Taxa_",
no.models, "SDMs_",
paste(names(list.noise), collapse = "_")
)
print(experiment.name)
# create output directory for experiment results
dir.experiment          <- paste0(dir.output, experiment.name, "/")
dir.metadata            <- paste0(dir.experiment, "metadata.json")
dir.create(dir.experiment)
# saving metadata to JSON file
metadata.list           <- list("input_data"       = file.input.data,
"prev_taxa"        = file.prev.taxa,
"experiment.name"  = experiment.name,
"number_split"     = number.split,
"split.criterion"  = split.criterion,
"number.sample"    = number.sample,
"models"           = models,
"env_factor"       = env.factor,
"env_factor_full"  = env.factor.full,
"noise"            = list.noise)
metadata.json           <- toJSON(metadata.list)
write(metadata.json, dir.metadata)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PREPROCESS DATA ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# preprocess input dataset
data.input <- data.env.taxa[,c(vect.info,env.factor,taxa.colnames)] # subselect columns of interest
data.input[which(is.na(data.input$MonitoringProgram)), "MonitoringProgram"] <- "SyntheticPoint" # replace empty monitoring programs
catch.variable <- "Watershed"
# sanity check of NAs
sum(is.na(data.input[,c(vect.info,env.factor)])) # should be zero
sum(is.na(data.input[,c(taxa.colnames)])) # could have a lot, decide to replace by zero or not
if("noise.disp" %in% names(list.noise)){
data.input[is.na(data.input)] <- 0 # replace NAs by 0
}
# replace 0/1 by absent/present
for (taxon in taxa.colnames) {
data.input[which(data.input[,taxon] == 0),taxon] <- "Absent"
data.input[which(data.input[,taxon] == 1),taxon] <- "Present"
data.input[,taxon] = as.factor(data.input[,taxon])
}
# make long dataframe for analysis
long.df.input.taxa <- gather(data.input, key = Taxon, value = Observation, all_of(taxa.colnames))
long.df.input.taxa$Taxon <- gsub("Occurrence.", "", long.df.input.taxa$Taxon)
long.df.input.env.taxa <- gather(long.df.input.taxa, key = Factor, value = Value, all_of(env.factor))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Prevalence analysis ####
# plot bars pres/abs/NA
plot.data <- long.df.input.taxa
plot.data$Observation <- factor(plot.data$Observation, levels=c("Present", NA, "Absent"))
temp.taxa <- unique(plot.data$Taxon)
temp.catch <- unique(plot.data$Watershed)
# analyze distribution of prevalence and NAs across catchments
p <- ggplot(plot.data, aes(x = Taxon, fill = Observation))
p <- p + geom_bar(stat="count", position='stack')
p <- p + facet_wrap(~ Watershed, scales = "free_y")
p <- p + scale_fill_manual(values=vect.col.pres.abs)
p <- p + theme_bw()
p <- p + scale_x_discrete(limits = temp.taxa)
p <- p + theme(axis.text.x = element_text(angle=90))
p
# analyze distribution of prevalence and NAs across taxa
q <- ggplot(plot.data, aes(x = Watershed, fill = Observation))
q <- q + geom_bar(stat = "count", position = "stack")
q <- q + facet_wrap(~ Taxon, scales = "free_y")
q <- q + scale_fill_manual(values=vect.col.pres.abs)
q <- q + theme_bw()
q <- q + scale_x_discrete(limits = temp.catch)
q <- q + theme(axis.text.x = element_text(angle=90))
q
# print plots in pdf
file.name <- "GeomBar_PresAbs_TaxaCatch"
print.pdf.plots(list.plots = list(q,p), width = 12, height = 9, dir.output = dir.experiment, info.file.name = "", file.name = file.name,
png = F)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Split and standardize data ####
preprocessed.data.cv  <- preprocess.data(data=data.input,
env.fact=env.factor,
dir=dir.experiment,
split.type="CV",
nb.split=number.split,
splitting.criterion=split.criterion)
preprocessed.data.fit <- preprocess.data(data=data.input,
env.fact=env.factor,
dir=dir.experiment,
split.type="FIT")
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# APPLY MODELS ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Cross-validation ####
models.cv <- lapply(sdm.models, FUN = apply.caret.model,
preprocessed.data.cv, split.type = "CV",
taxa.colnames, env.factor, list.noise)
View(preprocessed.data.cv)
###############################################################################|
#                                                                              #
#  --- Study of the influence of noise on overfitting of machine learning ---  #
#            --- and statistical species distribution models. ---              #
#                                                                              #
#                          --- February 2023 ---                               #
#                                                                              #
#                   --- Emma Chollet, Gaspard FragniÃ¨re ---                    #
#                --- Andreas Scheidegger and Nele Schuwirth ---                #
#                                                                              #
#                      --- emma.chollet@eawag.ch ---                           #
#                                  l                                           #
###############################################################################|
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PRELIMINARIES ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Sys.setenv(LANG="EN")
set.seed(13)   # Always set seed to a lucky number
getwd()        # Show working directory. It needs to be the location of 'main.r'
rm(list=ls())  # Free work space
graphics.off() # Clean graphics display
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Libraries ####
if (!require("dplyr")){install.packages("dplyr"); library("dplyr")}                    # to sort, join, merge data
if ( !require("tidyr") ) { install.packages("tidyr"); library("tidyr") }               # to sort, join, merge data
if (!require("readr")){install.packages("readr"); library("readr")}
if (!require("jsonlite")){install.packages("jsonlite"); library("jsonlite")}
if (!require("pROC")){install.packages("pROC"); library("pROC")}  # to compute AUC
if ( !require("sf") ) { install.packages("sf"); library("sf") }                        # to read layers for plotting maps
# specific for Neural Network
if (!require("reticulate")){install.packages("reticulate"); library("reticulate")}
#install_miniconda()              # run this the very first time reticulate is installed
#install.packages("tensorflow")
library("tensorflow")
#install_tensorflow()             # run this line only when opening new R session
#install.packages("keras")
library("keras")
#install_keras()                  # run this line only when opening new R session
#use_condaenv()
# reticulate::install_python(version = '<version>')
# caret has to be loaded at the end to not cache function 'train'
if (!require("caret")){install.packages("caret"); library("caret")}
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Directories, files and functions ####
# define directories
dir.input.data          <- "../A3_outputs_streambugs/"
dir.output              <- "../B2_outputs_sdms/"
dir.utilities           <- "../00_utilities/"
# dir.create(dir.output)
# define files
name.streambugs.run     <- "10Catch_3081Sites_Prev0.15_10Yea_3651Steps"
file.input.data         <- paste0(name.streambugs.run, "_WideData_ResultsThreshPresAbs.csv")
file.prev.taxa          <- paste0(name.streambugs.run, "_PrevalenceTaxonomy_ThreshPresAbs.csv")
file.selected.taxa      <- "selected_taxa_analysis.csv"
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Load data and functions ####
# read data
data.env.taxa          <- read.csv(paste0(dir.input.data, file.input.data), header = T, sep = ";", stringsAsFactors = F)
data.prev.taxa      <- read.csv(paste0(dir.input.data, file.prev.taxa), header = T, sep = ";", stringsAsFactors = F)
data.selected.taxa  <- read.csv(paste0(dir.utilities, file.selected.taxa), header = T, sep = ";", stringsAsFactors = F)
# load functions
source(paste0(dir.utilities, "utilities_global.r"))
source("performances_assessment.r")
source("training_pipeline.r")
source("ml_models.r")
source("plotting.r")
# prepare inputs for plotting results on swiss maps
map.inputs      <- map.inputs(directory = paste0(dir.utilities,"swiss.map.gdb"))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Inputs, training options and noise scenarios ####
# environmental factors used for streambugs
vect.dir.env.fact <- c("Temperature"      = "tempmaxC",
"Flow velocity"    = "currentms",
"Toxic units"      = "orgmicropollTU",
"Saproby"          = "saprowqclass")
vect.indir.env.fact <- c("Shade"          = "shade",
"Litter input"   = "Lit_Inp" ,
"Phosphorus"     = "C_P",
"Nitrate"        = "C_N")
vect.info <- colnames(data.env.taxa)[1:5]
print(vect.info)
# environmental factors selected for sdms
env.factor <- vect.dir.env.fact
env.factor.full <- env.factor
no.env.fact <- length(env.factor)
# taxa list full
cind.taxa <- which(grepl("Occurrence.", colnames(data.env.taxa)))
vect.taxa.full <- colnames(data.env.taxa)[cind.taxa]
names(vect.taxa.full) <- gsub("Occurrence.", "", vect.taxa.full)
no.taxa.full <- length(vect.taxa.full)
print(vect.taxa.full)
# taxa list selected for analysis and above prevalence threshold
prev.threshold <- 0.1
temp.occ.taxa <- data.prev.taxa[which(data.prev.taxa$Prevalence > prev.threshold), "Occurrence.taxa"]
selected.taxa.analysis <- data.selected.taxa$Occurrence.taxa
taxa.colnames <- selected.taxa.analysis[which(selected.taxa.analysis %in% temp.occ.taxa)]
names(taxa.colnames) <- gsub("Occurrence.", "", taxa.colnames)
no.taxa <- length(taxa.colnames)
print(taxa.colnames)
# select colors for present/absent plots
vect.col.pres.abs <- c( "Present" = "#00BFC4", "Absent" = "#F8766D")
scales::show_col(vect.col.pres.abs)
# sdms training options
number.split            <- 3
split.criterion         <- "ReachID"
number.sample           <- 1000
sdm.models              <- list("GLM" = "glm",
"GAM" = "gamLoess",
"RF"  = "rf")#,
# "ann")
no.models <- length(sdm.models)
models <- append(list("null"), sdm.models)
# noise scenarios
list.noise <- list(
# "noise.disp"     = list("type"       = "dispersal",
#                        "target"     = NULL,
#                        "amount"     = NULL,
#                        "parameters" = NULL) # ,
# "noise.temp"     = list("type"       = "gaussian",
#                         "target"     = "temperature",
#                         "amount"     = 5,
#                         "parameters" = list("min"=0, "max"=35)),
#
# "noise.gamm"     = list("type"       = "missdetection",
#                         "target"     = "Occurrence.Gammaridae",
#                         "amount"     = 0.1,
#                         "parameters" = NULL),
#
# "noise.add.fact" = list("type"       = "add_factor",
#                         "target"     = "random1", # new factor name
#                         "amount"     = NULL,
#                         "parameters" = NULL),
#
# "noise.rem.fact" = list("type"       = "remove_factor",
#                         "target"     = "temperature",
#                         "amount"     = NULL,
#                         "parameters" = NULL),
)
# define experiment name
experiment.name <- paste0(
number.sample, "Sites_",
no.taxa, "Taxa_",
no.models, "SDMs_",
paste(names(list.noise), collapse = "_")
)
print(experiment.name)
# create output directory for experiment results
dir.experiment          <- paste0(dir.output, experiment.name, "/")
dir.metadata            <- paste0(dir.experiment, "metadata.json")
dir.create(dir.experiment)
# saving metadata to JSON file
metadata.list           <- list("input_data"       = file.input.data,
"prev_taxa"        = file.prev.taxa,
"experiment.name"  = experiment.name,
"number_split"     = number.split,
"split.criterion"  = split.criterion,
"number.sample"    = number.sample,
"models"           = models,
"env_factor"       = env.factor,
"env_factor_full"  = env.factor.full,
"noise"            = list.noise)
metadata.json           <- toJSON(metadata.list)
write(metadata.json, dir.metadata)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PREPROCESS DATA ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# preprocess input dataset
data.input <- data.env.taxa[,c(vect.info,env.factor,taxa.colnames)] # subselect columns of interest
data.input[which(is.na(data.input$MonitoringProgram)), "MonitoringProgram"] <- "SyntheticPoint" # replace empty monitoring programs
catch.variable <- "Watershed"
# sanity check of NAs
sum(is.na(data.input[,c(vect.info,env.factor)])) # should be zero
sum(is.na(data.input[,c(taxa.colnames)])) # could have a lot, decide to replace by zero or not
if("noise.disp" %in% names(list.noise)){
data.input[is.na(data.input)] <- 0 # replace NAs by 0
}
# replace 0/1 by absent/present
for (taxon in taxa.colnames) {
data.input[which(data.input[,taxon] == 0),taxon] <- "Absent"
data.input[which(data.input[,taxon] == 1),taxon] <- "Present"
data.input[,taxon] = as.factor(data.input[,taxon])
}
# make long dataframe for analysis
long.df.input.taxa <- gather(data.input, key = Taxon, value = Observation, all_of(taxa.colnames))
long.df.input.taxa$Taxon <- gsub("Occurrence.", "", long.df.input.taxa$Taxon)
long.df.input.env.taxa <- gather(long.df.input.taxa, key = Factor, value = Value, all_of(env.factor))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Prevalence analysis ####
# plot bars pres/abs/NA
plot.data <- long.df.input.taxa
plot.data$Observation <- factor(plot.data$Observation, levels=c("Present", NA, "Absent"))
temp.taxa <- unique(plot.data$Taxon)
temp.catch <- unique(plot.data$Watershed)
# analyze distribution of prevalence and NAs across catchments
p <- ggplot(plot.data, aes(x = Taxon, fill = Observation))
p <- p + geom_bar(stat="count", position='stack')
p <- p + facet_wrap(~ Watershed, scales = "free_y")
p <- p + scale_fill_manual(values=vect.col.pres.abs)
p <- p + theme_bw()
p <- p + scale_x_discrete(limits = temp.taxa)
p <- p + theme(axis.text.x = element_text(angle=90))
p
# analyze distribution of prevalence and NAs across taxa
q <- ggplot(plot.data, aes(x = Watershed, fill = Observation))
q <- q + geom_bar(stat = "count", position = "stack")
q <- q + facet_wrap(~ Taxon, scales = "free_y")
q <- q + scale_fill_manual(values=vect.col.pres.abs)
q <- q + theme_bw()
q <- q + scale_x_discrete(limits = temp.catch)
q <- q + theme(axis.text.x = element_text(angle=90))
q
# print plots in pdf
file.name <- "GeomBar_PresAbs_TaxaCatch"
print.pdf.plots(list.plots = list(q,p), width = 12, height = 9, dir.output = dir.experiment, info.file.name = "", file.name = file.name,
png = F)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Split and standardize data ####
preprocessed.data.cv  <- preprocess.data(data=data.input,
env.fact=env.factor,
dir=dir.experiment,
split.type="CV",
nb.split=number.split,
splitting.criterion=split.criterion)
preprocessed.data.fit <- preprocess.data(data=data.input,
env.fact=env.factor,
dir=dir.experiment,
split.type="FIT")
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# APPLY MODELS ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Cross-validation ####
models.cv <- lapply(sdm.models, FUN = apply.caret.model,
preprocessed.data.cv, split.type = "CV",
taxa.colnames, env.factor, list.noise)
warnings()
