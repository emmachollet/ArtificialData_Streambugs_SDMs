return(-Inf) # then sampler will go away from these param values (instead of crashing)
} else {
lp = lp + loglikeli(par, system=system, obs=obs, verbose=verbose)
return(lp)
}
}
par.ini <- c(k.gro.ALG = 0.5, k.death.ALG = 0.1, K.HPO4 = 0.002,
sd.obs.HPO4 = 0.004, sd.obs.ALG = 0.02)
# test postior function
logprior(par = par.ini)
logposterior(par = par.ini, system=system, obs=observations)
# run sampler
sampsize <- 1000
res.mcmc <- MCMC(logposterior, n=sampsize,
system=system, obs=observations,
init = par.ini,  acc.rate = 0.234,
scale = 0.00001*par.ini)
res.mcmc$acceptance.rate
par(mfrow=c(2, 3))
for(i in 1:ncol(res.mcmc$samples)) {
plot(res.mcmc$samples[,i], type="l", main=colnames(res.mcmc$samples)[i], xlab="iteration"[i])
}
# install_miniconda()              # run this the very first time reticulate is installed # makes a separeta python installation
install.packages("tensorflow")
# virtualenv_list()
# virtualenv_exists(envname = NULL)
library("tensorflow")
# tensorflow::tf_config() # Check if TensorFlow is installed and accessible
# virtualenv_install("C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow", "tensorflow==2.16")
# install_tensorflow(envname = "C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow")
# virtualenv_remove("r-tensorflow")
install_tensorflow()             # run this line only when opening new R session
# install_tensorflow(envname = "C:/r-tensorflow")
# install.packages("keras")
library("keras")
install_keras()                  # run this line only when opening new R session
###############################################################################|
#                                                                              #
#  --- Study of the influence of noise on overfitting of machine learning ---  #
#            --- and statistical species distribution models. ---              #
#                                                                              #
#                          --- February 2023 ---                               #
#                                                                              #
#                   --- Emma Chollet, Gaspard Fragnière ---                    #
#                --- Andreas Scheidegger and Nele Schuwirth ---                #
#                                                                              #
#                      --- emma.chollet@eawag.ch ---                           #
#                                                                              #
###############################################################################|
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PRELIMINARIES ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Sys.setenv(LANG="EN")
set.seed(13)   # Always set seed to a lucky number
getwd()        # Show working directory. It needs to be the location of 'main.r'
rm(list=ls())  # Free work space
graphics.off() # Clean graphics display
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Libraries ####
if (!require("dplyr")){install.packages("dplyr"); library("dplyr")}                    # to sort, join, merge data
if (!require("tidyr") ) { install.packages("tidyr"); library("tidyr") }               # to sort, join, merge data
if (!require("ggplot2")){install.packages("ggplot2"); library("ggplot2")}                    # to sort, join, merge data
if (!require("readr")){install.packages("readr"); library("readr")}
if (!require("jsonlite")){install.packages("jsonlite"); library("jsonlite")}
if (!require("pROC")){install.packages("pROC"); library("pROC")}  # to compute AUC
if (!require("vtable") ) { install.packages("vtable"); library("vtable") }               # to do tables of summary statistics
if (!require("sf") ) { install.packages("sf"); library("sf") }                        # to read layers for plotting maps
if (!require("ggpattern")){install.packages("ggpattern"); library("ggpattern")}        # to add patterns in boxplots
# specific for Neural Network
if (!require("reticulate")){install.packages("reticulate"); library("reticulate")}    # links R to python
# install_miniconda()              # run this the very first time reticulate is installed # makes a separeta python installation
# install.packages("tensorflow")
# virtualenv_list()
# virtualenv_exists(envname = NULL)
# library("tensorflow")
# tensorflow::tf_config() # Check if TensorFlow is installed and accessible
# virtualenv_install("C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow", "tensorflow==2.16")
# install_tensorflow(envname = "C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow")
# virtualenv_remove("r-tensorflow")
# install_tensorflow()             # run this line only when opening new R session
# install_tensorflow(envname = "C:/r-tensorflow")
# install.packages("keras")
# library("keras")
# install_keras()                  # run this line only when opening new R session
# use_condaenv()
# reticulate::install_python(version = '<version>')
# path <-"C:/Users/cholleem/AppData/Local/r-miniconda/envs/r-reticulate/python.exe"
# Sys.setenv(RETICULATE_PYTHON = path)
# virtualenv_create("r-tensorflow")
# caret has to be loaded at the end to not cache function 'train'
if (!require("mgcv")){install.packages("mgcv"); library("mgcv")}
if (!require("caret")){install.packages("caret"); library("caret")}
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Directories and files ####
# define directories
dir.input.data          <- "../A3_outputs_streambugs/"
dir.output              <- "../B2_outputs_sdms/"
dir.utilities           <- "../00_utilities/"
# dir.create(dir.output)
# define files
name.streambugs.run     <- "8Catch_3009Sites_PolyInterp30_runC_100Yea_36501Steps"
file.input.data         <- paste0(name.streambugs.run, "_WideData_ResultsSamplePresAbs.csv")
file.prev.taxa          <- paste0(name.streambugs.run, "_PrevalenceTaxonomy_SamplePresAbs.csv")
file.selected.taxa      <- "selected_taxa_analysis.csv"
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Load data and functions ####
# read data
data.env.taxa       <- read.csv(paste0(dir.input.data, file.input.data), header = T, sep = ";", stringsAsFactors = F)
setwd("C:/Users/cholleem/Documents/ArtificialData_Streambugs_SDMs/B1_scripts_sdms")
###############################################################################|
#                                                                              #
#  --- Study of the influence of noise on overfitting of machine learning ---  #
#            --- and statistical species distribution models. ---              #
#                                                                              #
#                          --- February 2023 ---                               #
#                                                                              #
#                   --- Emma Chollet, Gaspard Fragnière ---                    #
#                --- Andreas Scheidegger and Nele Schuwirth ---                #
#                                                                              #
#                      --- emma.chollet@eawag.ch ---                           #
#                                                                              #
###############################################################################|
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PRELIMINARIES ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Sys.setenv(LANG="EN")
set.seed(13)   # Always set seed to a lucky number
getwd()        # Show working directory. It needs to be the location of 'main.r'
rm(list=ls())  # Free work space
graphics.off() # Clean graphics display
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Libraries ####
if (!require("dplyr")){install.packages("dplyr"); library("dplyr")}                    # to sort, join, merge data
if (!require("tidyr") ) { install.packages("tidyr"); library("tidyr") }               # to sort, join, merge data
if (!require("ggplot2")){install.packages("ggplot2"); library("ggplot2")}                    # to sort, join, merge data
if (!require("readr")){install.packages("readr"); library("readr")}
if (!require("jsonlite")){install.packages("jsonlite"); library("jsonlite")}
if (!require("pROC")){install.packages("pROC"); library("pROC")}  # to compute AUC
if (!require("vtable") ) { install.packages("vtable"); library("vtable") }               # to do tables of summary statistics
if (!require("sf") ) { install.packages("sf"); library("sf") }                        # to read layers for plotting maps
if (!require("ggpattern")){install.packages("ggpattern"); library("ggpattern")}        # to add patterns in boxplots
# specific for Neural Network
if (!require("reticulate")){install.packages("reticulate"); library("reticulate")}    # links R to python
# install_miniconda()              # run this the very first time reticulate is installed # makes a separeta python installation
# install.packages("tensorflow")
# virtualenv_list()
# virtualenv_exists(envname = NULL)
# library("tensorflow")
# tensorflow::tf_config() # Check if TensorFlow is installed and accessible
# virtualenv_install("C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow", "tensorflow==2.16")
# install_tensorflow(envname = "C:/Users/ClientAdmin/Documents/.virtualenvs/r-tensorflow")
# virtualenv_remove("r-tensorflow")
# install_tensorflow()             # run this line only when opening new R session
# install_tensorflow(envname = "C:/r-tensorflow")
# install.packages("keras")
# library("keras")
# install_keras()                  # run this line only when opening new R session
# use_condaenv()
# reticulate::install_python(version = '<version>')
# path <-"C:/Users/cholleem/AppData/Local/r-miniconda/envs/r-reticulate/python.exe"
# Sys.setenv(RETICULATE_PYTHON = path)
# virtualenv_create("r-tensorflow")
# caret has to be loaded at the end to not cache function 'train'
if (!require("mgcv")){install.packages("mgcv"); library("mgcv")}
if (!require("caret")){install.packages("caret"); library("caret")}
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Directories and files ####
# define directories
dir.input.data          <- "../A3_outputs_streambugs/"
dir.output              <- "../B2_outputs_sdms/"
dir.utilities           <- "../00_utilities/"
# dir.create(dir.output)
# define files
name.streambugs.run     <- "8Catch_3009Sites_PolyInterp30_runC_100Yea_36501Steps"
file.input.data         <- paste0(name.streambugs.run, "_WideData_ResultsSamplePresAbs.csv")
file.prev.taxa          <- paste0(name.streambugs.run, "_PrevalenceTaxonomy_SamplePresAbs.csv")
file.selected.taxa      <- "selected_taxa_analysis.csv"
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Load data and functions ####
# read data
data.env.taxa       <- read.csv(paste0(dir.input.data, file.input.data), header = T, sep = ";", stringsAsFactors = F)
data.prev.taxa      <- read.csv(paste0(dir.input.data, file.prev.taxa), header = T, sep = ";", stringsAsFactors = F)
data.selected.taxa  <- read.csv(paste0(dir.utilities, file.selected.taxa), header = T, sep = ";", stringsAsFactors = F)
# load functions
source(paste0(dir.utilities, "utilities_global.r"))
source("data_noising.r")
source("performances_assessment.r")
source("training_pipeline.r")
source("ml_models.r")
source("global_variables.r")
source("plotting.r")
# prepare inputs for plotting results on swiss maps
map.inputs      <- map.inputs(directory = paste0(dir.utilities,"swiss.map.gdb"))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Inputs, training options and noise scenarios ####
# environmental factors used for streambugs
vect.dir.env.fact <- c("Temperature"      = "tempmaxC",
"Flow velocity"    = "currentms",
"Toxic units"      = "orgmicropollTU",
"Saproby"          = "saprowqclass")
vect.indir.env.fact <- c("Shade"          = "shade",
"Litter input"   = "Lit_Inp" ,
"Phosphorus"     = "C_P",
"Nitrate"        = "C_N")
vect.info <- colnames(data.env.taxa)[1:5]
print(vect.info)
# environmental factors selected for sdms
# env.factor <- vect.dir.env.fact
env.factor <- c(vect.dir.env.fact, vect.indir.env.fact)
env.factor.full <- c(env.factor, "Temp2" = "tempmaxC2", "FV2" = "currentms2")
no.env.fact <- length(env.factor)
# taxa list full
cind.taxa <- which(grepl("Occurrence.", colnames(data.env.taxa)))
vect.taxa.full <- colnames(data.env.taxa)[cind.taxa]
names(vect.taxa.full) <- gsub("Occurrence.", "", vect.taxa.full)
no.taxa.full <- length(vect.taxa.full)
print(vect.taxa.full)
# update number of NAs in input data
for (taxon in vect.taxa.full) {
# taxon <- vect.taxa.full[1]
data.prev.taxa[which(data.prev.taxa$Occurrence.taxa == taxon), "Missing.values"] <- sum(is.na(data.env.taxa[,taxon]))
}
summary(data.prev.taxa$Missing.values)
# taxa list selected for analysis and above prevalence threshold
prev.threshold <- 0.1
na.threshold <- 1000
temp.occ.taxa <- data.prev.taxa[which(data.prev.taxa$Prevalence.NoDisp > prev.threshold
& data.prev.taxa$Prevalence.NoDisp < 1 -prev.threshold
& data.prev.taxa$Missing.values < na.threshold), "Occurrence.taxa"]
selected.taxa.analysis <- data.selected.taxa$Occurrence.taxa
# taxa.colnames <- selected.taxa.analysis[which(selected.taxa.analysis %in% temp.occ.taxa)]
taxa.colnames <- temp.occ.taxa
names(taxa.colnames) <- gsub("Occurrence.", "", taxa.colnames)
no.taxa <- length(taxa.colnames)
print(no.taxa)
print(taxa.colnames)
# select colors for present/absent plots
vect.col.pres.abs <- c( "Present" = "#00BFC4", "Absent" = "#F8766D")
scales::show_col(vect.col.pres.abs)
# sdms training options
number.split            <- 3
split.criterion         <- "ReachID"
number.sample           <- 3000
number.sample           <- ifelse(dim(data.env.taxa)[1] < number.sample, dim(data.env.taxa)[1], number.sample)
sdm.models              <- c(
"GLM" = "glm",
# "GAM" = "gam",
"GAM" = "gamSpline",
"RF"  = "rf",
"ANN" = "ann")#,
# "ann")
no.models <- length(sdm.models)
models <- append(c("Null" = "null"), sdm.models)
# noise scenarios
list.noise <- list(
# "noise.disp"     = list("type"       = "dispersal",
#                        "target"     = NULL,
#                        "amount"     = NULL,
#                        "parameters" = NULL) # ,
# "noise.temp"     = list("type"       = "gaussian",
#                         "target"     = "tempmaxC",
#                         "amount"     = 3,
#                         "parameters" = list("min"=0, "max"=30)) #,
# #
# "noise.gamm"     = list("type"       = "missdetection",
#                         "target"     = "Occurrence.Gammaridae",
#                         "amount"     = 0.1,
#                         "parameters" = NULL)#,
# #
# "noise.add.fact" = list("type"       = "add_factor",
#                         "target"     = "random1", # new factor name
#                         "amount"     = NULL,
#                         "parameters" = NULL),
#
# "noise.rem.fact" = list("type"       = "remove_factor",
#                         "target"     = "currentms",
#                         "amount"     = NULL,
#                         "parameters" = NULL)#,
)
name.list.noise <- paste(names(list.noise), collapse = "_")
p <- 0.1 # probability at which a presence is turned into an absence
list.noise <- lapply(taxa.colnames, FUN=function(taxon){
noise_taxon <- list("type"       = "missdetection",
"target"     = taxon,
"amount"     = p,
"parameters" = NULL)
return(noise_taxon)
})
name.list.noise <- paste0("misdetection.all.taxa", p)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PREPROCESS DATA ####
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Add noise ####
# preprocess input dataset
data.input <- data.env.taxa
data.input$tempmaxC2 <- data.input$tempmaxC^2
data.input$currentms2 <- data.input$currentms^2
data.input <- data.input[,c(vect.info,env.factor.full,taxa.colnames)] # subselect columns of interest
catch.variable <- "Watershed"
# add noise
data.input.noised        <- add.noise(data = data.input,
number.sample,
noise = list.noise,
env.fact = env.factor,
env.fact.full = env.factor.full)
data.input      <- data.input.noised$`noised data`
env.factor      <- data.input.noised$`env fact`
env.factor.full <- data.input.noised$`env fact full`
no.env.fact <- length(env.factor)
# reduce dataset to selected number of samples
rind.select <- runif(n=number.sample, min=1, max=dim(data.input)[1])
data.input <- data.input[rind.select,]
# sanity check of NAs
sum(is.na(data.input[,c(vect.info,env.factor)])) # should be zero
sum(is.na(data.input[,c(taxa.colnames)])) # could have a lot, decide to replace by zero or not
# replace NAs by 0 to simulate noise due to dispersal limitation
if("noise.disp" %in% names(list.noise)){
data.input[is.na(data.input)] <- 0 # replace NAs by 0
}
# replace 0/1 by absent/present
for (taxon in taxa.colnames) {
data.input[which(data.input[,taxon] == 0),taxon] <- "Absent"
data.input[which(data.input[,taxon] == 1),taxon] <- "Present"
data.input[,taxon] = as.factor(data.input[,taxon])
}
# # make long dataframe for analysis
# long.df.input.taxa <- gather(data.input, key = Taxon, value = Observation, all_of(taxa.colnames))
# long.df.input.taxa$Taxon <- gsub("Occurrence.", "", long.df.input.taxa$Taxon)
# long.df.input.env.taxa <- gather(long.df.input.taxa, key = Factor, value = Value, all_of(env.factor))
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Write metadata ####
# define experiment name
experiment.name <- paste0(
number.sample, "Sites_",
no.taxa, "Taxa_",
no.models, "SDMs_",
no.env.fact, "EnvFact_",
name.list.noise
)
print(experiment.name)
# create output directory for experiment results
dir.experiment          <- paste0(dir.output, experiment.name, "/")
dir.metadata            <- paste0(dir.experiment, "metadata.json")
dir.create(dir.experiment)
# saving metadata to JSON file
metadata.list           <- list("input_data"       = file.input.data,
"prev_taxa"        = file.prev.taxa,
"experiment.name"  = experiment.name,
"number_split"     = number.split,
"split.criterion"  = split.criterion,
"number.sample"    = number.sample,
"models"           = models,
"env_factor"       = env.factor,
"env_factor_full"  = env.factor.full,
"noise"            = list.noise)
metadata.json           <- toJSON(metadata.list)
write(metadata.json, dir.metadata)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Prevalence analysis ####
# # plot bars pres/abs/NA
# plot.data <- long.df.input.taxa
# plot.data$Observation <- factor(plot.data$Observation, levels=c("Present", NA, "Absent"))
# temp.taxa <- unique(plot.data$Taxon)
# temp.catch <- unique(plot.data$Watershed)
#
# # analyze distribution of prevalence and NAs across catchments
# p <- ggplot(plot.data, aes(x = Taxon, fill = Observation))
# p <- p + geom_bar(stat="count", position='stack')
# p <- p + facet_wrap(~ Watershed, scales = "free_y")
# p <- p + scale_fill_manual(values=vect.col.pres.abs)
# p <- p + theme_bw()
# p <- p + scale_x_discrete(limits = temp.taxa)
# p <- p + theme(axis.text.x = element_text(angle=90))
# p
#
# # analyze distribution of prevalence and NAs across taxa
# q <- ggplot(plot.data, aes(x = Watershed, fill = Observation))
# q <- q + geom_bar(stat = "count", position = "stack")
# q <- q + facet_wrap(~ Taxon, scales = "free_y")
# q <- q + scale_fill_manual(values=vect.col.pres.abs)
# q <- q + theme_bw()
# q <- q + scale_x_discrete(limits = temp.catch)
# q <- q + theme(axis.text.x = element_text(angle=90))
# q
#
# # print plots in pdf
# file.name <- "GeomBar_PresAbs_TaxaCatch"
# print.pdf.plots(list.plots = list(q,p), width = 12, height = 9, dir.output = dir.experiment, info.file.name = "", file.name = file.name,
#                 png = F)
#
# plot.data <- filter(plot.data, Taxon %in% "Gammaridae")
# q <- ggplot(plot.data, aes(x = Watershed, fill = Observation))
# q <- q + geom_bar(stat = "count", position = "stack")
# q <- q + scale_fill_manual(values=vect.col.pres.abs)
# q <- q + theme_bw()
# q <- q + scale_x_discrete(limits = temp.catch)
# q <- q + theme(axis.text.x = element_text(angle=90))
# q
# file.name <- "GeomBar_PresAbs_GammaridaeCatch"
# print.pdf.plots(list.plots = list(q), width = 7, height = 5, dir.output = dir.experiment, info.file.name = "", file.name = file.name,
#                 png = F)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Split and standardize data ####
preprocessed.data.cv  <- preprocess.data(data=data.input,
env.fact=env.factor,
dir=dir.experiment,
split.type="CV",
nb.split=number.split,
splitting.criterion=split.criterion)
preprocessed.data.fit <- preprocess.data(data=data.input,
env.fact=env.factor,
dir=dir.experiment,
split.type="FIT")
# recover Streambugs data for ICE
data.base.ice       <- read.csv(paste0(dir.input.data, "WideDataInputs_ICE_50Sites.csv"), sep = ";")
data.base.ice$tempmaxC2 <- data.base.ice$tempmaxC^2
data.base.ice$currentms2 <- data.base.ice$currentms^2
name.ice.streambugs <- "8Catch_50Sites_ICE_50Steps_PolyInterp30_runC_100Yea_36501Steps_"
ice.df.streambugs   <- read.csv(paste0(dir.input.data, name.ice.streambugs,
"WideData_ResultsProbObs.csv"), sep = ";")
no.sites <- as.numeric(stringr::str_match(name.ice.streambugs, "Catch_(.+)Sites")[2])
no.steps <- as.numeric(stringr::str_match(name.ice.streambugs, "ICE_(.+)Steps_Poly")[2])
# check balance presence/absence
for(i.split in 1:number.split){
cat("Split number:", i.split, "\n")
for (taxon in taxa.colnames) {
cat(taxon)
print(summary(preprocessed.data.cv[[i.split]][["Training data"]][,taxon]))
}
}
print(summary(preprocessed.data.cv[[1]][["Training data"]][,"Occurrence.Baetisalpinus"]))
# Absent Present
# 1149     836
# options for plots comparison
# create directory for comparison plots
dir.compar.plots <- paste0(dir.output, "comparison_plots/")
dir.create(dir.compar.plots)
model.color.map <- c('GLM'     = "#619CFF",  # 'deepskyblue',   # Generalized Linear Model
'GAM'     = "#00BA38", # 'green',         # Generalized Additive Model
'ANN'     = 'orange',        # Artificial Neural Network
'RF'      = "#F8766D") # 'red')#,           # Random Forest
# 'Null'          = 'black')         # Null Model
scales::show_col(model.color.map)
# select taxon and env. factor
taxon.under.obs <- names(taxa.colnames)[5]
select.env.fact <- env.factor[1]
name.select.env.fact <- names(select.env.fact)
print(taxon.under.obs)
print(select.env.fact)
# width and height of an A4 page in inches
width.a4 = 8.3
height.a4 = 11.7
mytheme <- theme_bw() +
theme(text = element_text(size = 14),
title = element_text(size = 14),
# plot.title = element_text(face = "bold"),
# strip.text = element_text(size = 14), #, face = "bold"),
strip.background = element_rect(fill = "white"),
# axis.title = element_text(size = 14), #, face = "bold"),
# axis.text.x = element_text(vjust = 10),
# axis.text.y = element_text(hjust = 10),
# axis.title.x.bottom = element_text(vjust = -15),
# axis.title.x = element_text(margin = margin(t = 0, r = 0, b = -30, l = 0)), # put some space between the axis title and the numbers
axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
legend.position="bottom"
# legend.text = element_text(size = 14),
# legend.title = element_text(size = 14) #, face= "bold") #,
# panel.grid.major = element_line(colour = NA),
# panel.grid.minor = element_line(colour = NA)
)
theme_set(mytheme)
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
## Comparison different noise scenarios ----
# compare experiment dispersal noise
list.exp     <- list("Best case scenario"                  = "3000Sites_45Taxa_4SDMs_8EnvFact_",
"Reduce dataset size"                 = "300Sites_45Taxa_4SDMs_8EnvFact_",
"Remove environmental predictor"      = "3000Sites_45Taxa_4SDMs_7EnvFact_noise.rem.fact",
"Noise on temperature"                = "3000Sites_45Taxa_4SDMs_8EnvFact_noise.temp",
"Misdetection"                        = "3000Sites_45Taxa_4SDMs_8EnvFact_misdetection.all.taxa0.1",
"Noise dispersal limitation"          = "3000Sites_45Taxa_4SDMs_8EnvFact_noise.disp")
# test.colors <- RColorBrewer::brewer.pal(8, "Set1")
# print(test.colors)
# scales::show_col(test.colors)
# color.map <- RColorBrewer::brewer.pal(length(list.exp), "Dark2")
# scales::show_col(color.map)
color.map <- c('1'            = '#4daf4a',
'2'            = '#377eb8',
'3'            = '#984ea3',
'4'            = '#e41a1c',
'5'            = '#ff7f00',
'6'            = '#ffd92f')
color.map <- color.map[1:length(list.exp)]
names(color.map) <- names(list.exp)
scales::show_col(color.map)
# create file names for saving plots
file.name.exp <- paste0("comparison_300points_rem.FV_noise.temp_misdet_disp.lim_",
length(list.exp), "exp_")
print(file.name.exp)
file.name.tax <- paste0(file.name.exp,
taxon.under.obs, "_",
select.env.fact)
print(file.name.tax)
# create.comparison.plots("gauss_temp", list.exp.gauss,
#                         file.prev.taxa="8Catch_1416Sites_curve.curr.temp-10_interc.orgmic.sapro4_10Yea_3651Steps_PrevalenceTaxonomy_ThreshPresAbs.csv",
#                         taxon="Gammaridae",
#                         env.fact="tempmaxC")
name <- list.exp[[2]]
# name <- list.exp[[2]]
dir.experiment          <- paste0(dir.output, name, "/")
cat("\nLoading models and computing ICE for taxon", taxon.under.obs, "and experiment:", name, "\n")
models.fit      <- load.models(path=dir.experiment, split.type="FIT")
